# üéØ Dedicated Vertex AI Endpoint Setup

## üöÄ **Your Dedicated Endpoint Integration is Ready!**

Based on your curl sample, I've created a specialized integration for your dedicated Vertex AI endpoint.

## üìã **Vercel Environment Variables**

Set these **exact values** in your Vercel dashboard (matching Vertex AI sample format):

```bash
# Required - Your Google Cloud configuration
GOOGLE_CLOUD_PROJECT_ID=lume-f4327
PROJECT_ID=lume-f4327
PROJECT_NUMBER=1075430485377
GOOGLE_CLOUD_PROJECT_NUMBER=1075430485377
GOOGLE_CLOUD_LOCATION=europe-west4

# Required - Your dedicated endpoint configuration (Vertex AI format)
ENDPOINT_ID=2159768491417141248
SERA_MODEL_ID=2159768491417141248
SERA_MODEL_NAME=PsychoCounsel LLaMA3 8B
SERA_ENDPOINT_URL=https://2159768491417141248.europe-west4-1075430485377.prediction.vertexai.goog

# Required - Your service account JSON key (already working!)
GOOGLE_SERVICE_ACCOUNT_KEY={your-existing-json-key}

# Optional - For UI display
NEXT_PUBLIC_SERA_MODEL_ID=2159768491417141248
NEXT_PUBLIC_SERA_MODEL_NAME=PsychoCounsel LLaMA3 8B
```

## ‚úÖ **Variable Mapping (Vertex AI ‚Üí Our App)**

From your Vertex AI sample:
- `ENDPOINT_ID="2159768491417141248"` ‚Üí `ENDPOINT_ID` and `SERA_MODEL_ID`
- `PROJECT_ID="1075430485377"` ‚Üí `PROJECT_NUMBER` and `GOOGLE_CLOUD_PROJECT_NUMBER`
- Your actual project ID ‚Üí `GOOGLE_CLOUD_PROJECT_ID` and `PROJECT_ID`

## ‚úÖ **What Changed**

1. **New Dedicated Service**: Created `lib/vertex-ai-dedicated-endpoint.ts` that works with your specific endpoint URL format
2. **Direct HTTP Calls**: Uses the same pattern as your curl command - direct HTTP requests with Bearer token
3. **Proper Authentication**: Uses Google Auth Library to get access tokens (like `gcloud auth print-access-token`)
4. **Updated API Route**: New `/api/sera-chat-dedicated` endpoint specifically for your dedicated endpoint
5. **UI Integration**: Sera now calls the dedicated endpoint automatically

## üîß **How It Works**

The integration now:
1. **Gets an access token** (like your curl command's `$(gcloud auth print-access-token)`)
2. **Makes POST request** to your exact endpoint URL
3. **Uses your endpoint's prediction format** with `instances` and `parameters`
4. **Handles your model's response format** automatically

## üß™ **Testing Steps**

### 1. **Update Vercel Environment Variables**
- Add the variables above to your Vercel dashboard
- Make sure `GOOGLE_CLOUD_LOCATION=europe-west4` (not us-central1!)

### 2. **Redeploy**
- Push your changes or redeploy in Vercel

### 3. **Health Check**
```bash
curl https://your-app.vercel.app/api/sera-chat-dedicated
```

**Expected Success Response:**
```json
{
  "status": "ok",
  "service": "dedicated-endpoint",
  "models": {
    "2159768491417141248": true
  },
  "config": {
    "projectId": "lume-f4327",
    "location": "europe-west4",
    "endpointId": "2159768491417141248",
    "endpointUrl": "https://2159768491417141248.europe-west4-1075430485377.prediction.vertexai.goog",
    "hasCredentials": true
  }
}
```

### 4. **Chat with Sera**
- Open your app
- Start chatting - Sera should now use your PsychoCounsel LLaMA3 model!

## üéØ **Key Differences from Standard Vertex AI**

- **URL Format**: Uses your dedicated endpoint URL instead of standard Vertex AI endpoints
- **Request Format**: Follows the `instances` and `parameters` structure from your curl example
- **Authentication**: Direct Bearer token authentication (same as your curl command)
- **Location**: Your endpoint is in `europe-west4`, not `us-central1`

## üîç **Troubleshooting**

### **If Health Check Fails:**

1. **Check the error message** in the `modelDetails` field
2. **Verify environment variables** are set correctly in Vercel
3. **Confirm endpoint URL** matches exactly: `https://2159768491417141248.europe-west4-1075430485377.prediction.vertexai.goog`
4. **Check service account permissions** - needs access to your dedicated endpoint

### **If Chat Doesn't Work:**

1. **Check browser console** for error messages
2. **Test health endpoint first** to ensure basic connectivity
3. **Verify response format** - the integration handles multiple response formats automatically

## üéâ **Success Indicators**

When working correctly:
- ‚úÖ Health check returns `"status": "ok"`
- ‚úÖ Models shows `"2159768491417141248": true`
- ‚úÖ Sera responds using your PsychoCounsel model
- ‚úÖ Responses are contextual and therapeutic
- ‚úÖ Voice integration works (Sera speaks responses)

## üìû **Your Model is Now Live!**

Once you update the environment variables and redeploy, Sera will be powered by your custom PsychoCounsel LLaMA3 8B model deployed on Vertex AI!

The integration is specifically designed for your dedicated endpoint format and should work seamlessly with your existing authentication setup.